replay_size: 100000
gamma: 0.9
lr: &lr_val 1.0e-4
train_every: 10
train_batches: 1
update_after: 10000
batch_size: 128
total_steps: 5000000
steps_per_epoch: 10000
random_steps_fraction: 0.5
init_epsilon: 1
final_epsilon: 0.1
target_update: 1
target_update_every: 500

controller: 'DecentralizedController'
agent: 'DQNAgent'
true_reward: False
use_full_obs: False
device: 'cpu'

env_name_index: 1
N_agents: &n_agents 2
env_args: 
  N: *n_agents
  max_cycles: 100
  grid_width : 2
  grid_height : 9
  levers_prob_self : [[1, 0, 0, 0], [0.7, 0.2, 0.1, 0], [0, 0, 0, 1], [0, 0, 0, 1]]
  levers_prob_other : [[0.05, 0.9, 0.0, 0.05], [0.25, 0.05, 0.05, 0.65],[0, 0, 0.5, 0.5], [0, 0, 0, 1]] 
max_food: 2
hidden_layers: [32, 32]
eval_episodes: 2
render_last_episode: False

reward_hidden_layers: [32,32]
N_predictors: 3
perturb_prob: 0.0
reward_lr: *lr_val

trajectory_length: 10
pairs_count: 32
similarity: 0.25
reward_weighting: 0.7
