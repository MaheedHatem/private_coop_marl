replay_size: 100
trajectory_database: 10000
trajectory_buffer_ratio: 1
gamma: 0.99
lr: &lr_val 3.0e-4
train_every: 100
train_batches: 1
update_after: 1000
batch_size: 100
total_steps: 5000000
steps_per_epoch: 10000
value_coef: 0.5
entropy_coef: 0.01

controller: 'DecentralizedController'
agent: 'ACAgent'
true_reward: False
use_full_obs: False
device: 'cpu'

env_name_index: 1
N_agents: &n_agents 1
env_args: 
  N: *n_agents
  max_cycles: 100
  grid_width : 2
  grid_height : 9
  levers_prob_self : [[1, 0, 0, 0], [0, 1, 0, 0]]
  levers_prob_other : [[0, 0, 0, 1], [0, 0, 0, 1]] 
max_food: 2
hidden_layers: [32, 32]
eval_episodes: 2
render_last_episode: False

reward_hidden_layers: [32,32]
N_predictors: 3
perturb_prob: 0.0
reward_lr: *lr_val

trajectory_length: 10
pairs_count: 32
similarity: 0.25
reward_weighting: 0.7
